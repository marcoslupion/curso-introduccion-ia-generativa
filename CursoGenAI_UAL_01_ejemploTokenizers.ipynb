{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c371a522",
   "metadata": {},
   "source": [
    "# Curso de IA Generativa - Tokenizers\n",
    "### Uso de Tokenizers preentrenados y creación de uno desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49579b56",
   "metadata": {},
   "source": [
    "\n",
    "En este notebook, exploraremos cómo utilizar un tokenizer preentrenado para codificar una frase, visualizaremos los IDs y tokens asociados, y finalmente entrenaremos un tokenizer desde cero.\n",
    "Vamos a utilizar las herramientas proporcionadas por la biblioteca de Hugging Face `transformers`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afbad6",
   "metadata": {},
   "source": [
    "## Uso de un Tokenizer Preentrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9f5cd",
   "metadata": {},
   "source": [
    "\n",
    "Primero, vamos a utilizar un tokenizer preentrenado de Hugging Face para tokenizar una frase. Utilizaremos el modelo `bert-base-uncased` para este ejemplo.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb40bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9273d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', '.']\n",
      "Token IDs: [7976, 4454, 2003, 17903, 1996, 2088, 1012]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el tokenizer preentrenado\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Frase de ejemplo\n",
    "text = \"Artificial Intelligence is transforming the world.\"\n",
    "\n",
    "# Tokenización de la frase\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Mostrar tokens y sus IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f05860",
   "metadata": {},
   "source": [
    "## Visualización de los Tokens y sus IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990392cb",
   "metadata": {},
   "source": [
    "\n",
    "A continuación, visualizaremos los tokens y sus IDs correspondientes. Esto nos permite ver cómo el tokenizer descompone la frase en sub-palabras y asigna un identificador único a cada token.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a5b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(list(zip(tokens, token_ids)), columns=[\"Token\", \"Token ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebe8979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Token ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificial</td>\n",
       "      <td>7976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>intelligence</td>\n",
       "      <td>4454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transforming</td>\n",
       "      <td>17903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>world</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token  Token ID\n",
       "0    artificial      7976\n",
       "1  intelligence      4454\n",
       "2            is      2003\n",
       "3  transforming     17903\n",
       "4           the      1996\n",
       "5         world      2088\n",
       "6             .      1012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1403868",
   "metadata": {},
   "source": [
    "## Entrenamiento de un Tokenizer desde Cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3dfdb",
   "metadata": {},
   "source": [
    "Vamos a entrenar un tokenizer desde cero utilizando un pequeño corpus de datos de ejemplo. Para ello, usaremos la clase `ByteLevelBPETokenizer` de la librería `tokenizers`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1887f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un corpus de ejemplo\n",
    "corpus = [\n",
    "    \"Artificial intelligence and machine learning are evolving fields.\",\n",
    "    \"Natural language processing enables machines to understand human language.\",\n",
    "    \"Tokenizers are crucial for preparing text data for machine learning models.\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train([\"archivos/quijote.txt\"], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec64b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0e0dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " '##llo',\n",
       " ',',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'all',\n",
       " '!',\n",
       " '[UNK]',\n",
       " 'ar',\n",
       " '##e',\n",
       " 'yo',\n",
       " '##u',\n",
       " '[UNK]',\n",
       " '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ecb1e",
   "metadata": {},
   "source": [
    "Como se puede observar, hay algunos caracteres y sílabas de inglés que no se usan en el dataset del quijote.txt. Por eso, el tokenizer no es capaz de reconocerlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594e9cf",
   "metadata": {},
   "source": [
    "## Comparación de Tokenizers Preentrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bf6be",
   "metadata": {},
   "source": [
    "\n",
    "En este ejercicio, compararemos cómo diferentes tokenizers preentrenados (BERT, GPT-2 y RoBERTa) tokenizan la misma frase. Esto nos ayudará a entender cómo cada modelo interpreta y representa las palabras.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6649d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BERT\n",
      "Tokens: ['en', 'un', 'lu', '##gar', 'de', 'la', 'man', '##cha', ',', 'de', 'cu', '##yo', 'no', '##mbre', 'no', 'qui', '##ero', 'ac', '##ord', '##arm', '##e', '.']\n",
      "Número de Tokens: 22\n",
      "==================================================\n",
      "Tokenizer: GPT-2\n",
      "Tokens: ['En', 'Ġun', 'Ġl', 'ugar', 'Ġde', 'Ġla', 'ĠMan', 'cha', ',', 'Ġde', 'Ġc', 'uy', 'o', 'Ġn', 'omb', 're', 'Ġno', 'Ġqu', 'ier', 'o', 'Ġac', 'ord', 'ar', 'me', '.']\n",
      "Número de Tokens: 25\n",
      "==================================================\n",
      "Tokenizer: RoBERTa\n",
      "Tokens: ['En', 'Ġun', 'Ġl', 'ugar', 'Ġde', 'Ġla', 'ĠMan', 'cha', ',', 'Ġde', 'Ġc', 'uy', 'o', 'Ġn', 'omb', 're', 'Ġno', 'Ġqu', 'ier', 'o', 'Ġac', 'ord', 'ar', 'me', '.']\n",
      "Número de Tokens: 25\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Frase de ejemplo para comparar tokenizers\n",
    "sample_text = \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme.\"\n",
    "\n",
    "# Cargar tokenizers preentrenados\n",
    "tokenizers = {\n",
    "    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "}\n",
    "\n",
    "# Tokenizar la frase con cada tokenizer y mostrar los resultados\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"Tokenizer: {name}\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Número de Tokens:\", len(tokens))\n",
    "    print(\"=\"*50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1d7e4",
   "metadata": {},
   "source": [
    "## Análisis de la Longitud de las Secuencias Tokenizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7681b87",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a tokenizar frases de distintas longitudes utilizando los mismos tokenizers preentrenados (BERT, GPT-2 y RoBERTa). Observaremos cómo cambia la longitud de las secuencias resultantes según el tokenizer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6b6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Cervantes\n",
      "Tokenizer: BERT - Número de Tokens: 3\n",
      "Tokenizer: GPT-2 - Número de Tokens: 3\n",
      "Tokenizer: RoBERTa - Número de Tokens: 3\n",
      "==================================================\n",
      "Texto: En un lugar de la Mancha.\n",
      "Tokenizer: BERT - Número de Tokens: 9\n",
      "Tokenizer: GPT-2 - Número de Tokens: 9\n",
      "Tokenizer: RoBERTa - Número de Tokens: 9\n",
      "==================================================\n",
      "Texto: En un lugar de la Mancha, de cuyo nombre no quiero acordarme.\n",
      "Tokenizer: BERT - Número de Tokens: 22\n",
      "Tokenizer: GPT-2 - Número de Tokens: 25\n",
      "Tokenizer: RoBERTa - Número de Tokens: 25\n",
      "==================================================\n",
      "Texto: En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero.\n",
      "Tokenizer: BERT - Número de Tokens: 44\n",
      "Tokenizer: GPT-2 - Número de Tokens: 50\n",
      "Tokenizer: RoBERTa - Número de Tokens: 50\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Frases de diferentes longitudes\n",
    "texts = [\n",
    "    \"Cervantes\",\n",
    "    \"En un lugar de la Mancha.\",\n",
    "    \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme.\",\n",
    "    \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero.\"\n",
    "]\n",
    "\n",
    "# Comparar la longitud de secuencia para cada frase y cada tokenizer\n",
    "for text in texts:\n",
    "    print(f\"Texto: {text}\")\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"Tokenizer: {name} - Número de Tokens: {len(tokens)}\")\n",
    "    print(\"=\"*50)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
