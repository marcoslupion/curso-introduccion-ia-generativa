{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cde12d",
   "metadata": {},
   "source": [
    "\n",
    "# Ejemplo de Retrieval-Augmented Generation (RAG) con S-BERT y FAISS\n",
    "\n",
    "En este notebook, vamos a implementar un ejemplo de Retrieval-Augmented Generation (RAG) utilizando **Sentence-BERT (S-BERT)** para la generación de embeddings y **FAISS** para la búsqueda eficiente. \n",
    "\n",
    "RAG combina técnicas de búsqueda con generación de respuestas para mejorar la calidad de las interacciones de los chatbots. Primero, recuperamos los documentos más relevantes de una base de datos y luego utilizamos esa información para generar respuestas más precisas.\n",
    "\n",
    "El flujo de este ejemplo será el siguiente:\n",
    "1. Crearemos un dataset con información almacenada en un array.\n",
    "2. Generaremos embeddings usando S-BERT.\n",
    "3. Almacenaremos estos embeddings usando la biblioteca **FAISS** para una búsqueda eficiente.\n",
    "4. Crearemos un chatbot que solicitará al usuario una frase, generará su embedding y devolverá las respuestas más similares de la base de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51c806",
   "metadata": {},
   "source": [
    "## Instalación e importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instalación de las bibliotecas necesarias\n",
    "!pip install sentence-transformers faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdf50c",
   "metadata": {},
   "source": [
    "## Generacíon del dataset de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8fe96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados para el dataset: (7, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear un dataset simple con algunas frases de ejemplo\n",
    "dataset = [\n",
    "    \"La inteligencia artificial está revolucionando el mundo.\",\n",
    "    \"La tecnología de redes neuronales se usa en muchos campos.\",\n",
    "    \"El procesamiento del lenguaje natural es una rama de la inteligencia artificial.\",\n",
    "    \"Los sistemas de recomendación se basan en el aprendizaje automático.\",\n",
    "    \"El Internet de las Cosas está cambiando la forma en que vivimos.\",\n",
    "    \"La robótica es una combinación de ingeniería y tecnología avanzada.\",\n",
    "    \"La visión por computadora se utiliza para analizar imágenes y videos.\"\n",
    "]\n",
    "\n",
    "# Inicializar el modelo de S-BERT para la generación de embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Generar embeddings para cada frase en el dataset\n",
    "embeddings = model.encode(dataset)\n",
    "print(f\"Embeddings generados para el dataset: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683fadeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores almacenados en el índice: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear un índice de FAISS para la búsqueda eficiente usando L2 (distancia euclidiana)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Añadir los embeddings al índice\n",
    "index.add(embeddings)\n",
    "print(f\"Total de vectores almacenados en el índice: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8126775",
   "metadata": {},
   "source": [
    "## Creación y ejecución del chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a3dc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados más similares encontrados en el dataset:\n",
      "Frase: La inteligencia artificial está revolucionando el mundo. (Similitud: 26.722129821777344)\n",
      "Frase: Los sistemas de recomendación se basan en el aprendizaje automático. (Similitud: 35.01416778564453)\n",
      "Frase: El Internet de las Cosas está cambiando la forma en que vivimos. (Similitud: 40.01490020751953)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Función para interactuar con el chatbot\n",
    "def consultar_chatbot():\n",
    "    # Solicitar al usuario una frase para buscar en el dataset\n",
    "    consulta = input(\"Introduce una frase para buscar en el dataset: \")\n",
    "    consulta = \"¿Qué está revolucionando la IA?\"\n",
    "\n",
    "    # Generar el embedding de la frase del usuario\n",
    "    consulta_embedding = model.encode([consulta])\n",
    "\n",
    "    # Realizar la búsqueda en el índice FAISS para encontrar los elementos más similares\n",
    "    D, I = index.search(consulta_embedding, k=3)  # Obtener los 3 elementos más similares\n",
    "\n",
    "    # Mostrar los resultados al usuario\n",
    "    print(\"\\nResultados más similares encontrados en el dataset:\")\n",
    "    for i in range(len(I[0])):\n",
    "        print(f\"Frase: {dataset[I[0][i]]} (Similitud: {D[0][i]})\")\n",
    "\n",
    "# Ejecutar la función del chatbot\n",
    "consultar_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfb7b0",
   "metadata": {},
   "source": [
    "## Incorporación de un modelo generativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1bf3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Función para generar respuesta con GPT-2\n",
    "def generar_respuesta_gen(pregunta):\n",
    "    inputs = tokenizer(pregunta, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generar respuesta utilizando el attention_mask\n",
    "    outputs = model_gen.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=300,  # Permitir respuestas más largas\n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        temperature=0.7,  # Ajuste de la diversidad\n",
    "        top_p=0.9,  # Nucleus sampling\n",
    "        early_stopping=True\n",
    "    )\n",
    "    respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return respuesta\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-1b1\")\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-1b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "309561a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados más similares encontrados en el dataset:\n",
      "Frase: Los sistemas de recomendación se basan en el aprendizaje automático. (Similitud: 15.865922927856445)\n",
      "Frase: El Internet de las Cosas está cambiando la forma en que vivimos. (Similitud: 30.20244598388672)\n",
      "Frase: La robótica es una combinación de ingeniería y tecnología avanzada. (Similitud: 33.447967529296875)\n",
      "Consulta a BLOOMZ\n",
      "Contexto: Los sistemas de recomendación se basan en el aprendizaje automático.. Ahora responde a la pregunta: ¿En qué se basan los sistemas de recomendación?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta generada por BLOOMZ:\n",
      "Contexto: Los sistemas de recomendación se basan en el aprendizaje automático.. Ahora responde a la pregunta: ¿En qué se basan los sistemas de recomendación? aprendizaje\n"
     ]
    }
   ],
   "source": [
    "# Función para interactuar con el chatbot\n",
    "def consultar_chatbot():\n",
    "    # Solicitar al usuario una frase para buscar en el dataset\n",
    "    #consulta = input(\"Introduce una frase para buscar en el dataset: \")\n",
    "    consulta = \"¿En qué se basan los sistemas de recomendación?\"\n",
    "    \n",
    "    # Generar el embedding de la frase del usuario\n",
    "    consulta_embedding = model.encode([consulta])\n",
    "\n",
    "    # Realizar la búsqueda en el índice FAISS para encontrar los elementos más similares\n",
    "    D, I = index.search(consulta_embedding, k=3)  # Obtener los 3 elementos más similares\n",
    "\n",
    "    # Mostrar los resultados al usuario\n",
    "    print(\"\\nResultados más similares encontrados en el dataset:\")\n",
    "    for i in range(len(I[0])):\n",
    "        print(f\"Frase: {dataset[I[0][i]]} (Diferencia: {D[0][i]})\")\n",
    "\n",
    "    # Usar BLOOMZ para generar una respuesta basada en la consulta\n",
    "    contexto = dataset[I[0][0]]  # Utilizar la frase más relevante del dataset\n",
    "    consulta_gpt = f\"Contexto: {contexto}. Ahora responde a la pregunta: {consulta}\"\n",
    "    \n",
    "    print(\"Consulta a BLOOMZ\")\n",
    "    print(consulta_gpt)\n",
    "    \n",
    "    respuesta_bloomz = generar_respuesta_gen(consulta_gpt)\n",
    "    print(\"\\nRespuesta generada por BLOOMZ:\")\n",
    "    print(respuesta_bloomz)\n",
    "\n",
    "# Ejecutar la función del chatbot\n",
    "consultar_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
