{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d91e68a",
   "metadata": {},
   "source": [
    "# Entrenamiento de BERT con Masked Language Modelling (MLM) - CursoGenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0aa6e",
   "metadata": {},
   "source": [
    "\n",
    "En este notebook, utilizaremos la librería `transformers` de Hugging Face para cargar un modelo BERT sin entrenar y aplicaremos el Masked Language Modelling (MLM) utilizando el dataset de literatura de Cervantes. Nuestro objetivo será entrenar el modelo para que pueda predecir palabras enmascaradas en el texto.\n",
    "\n",
    "### Pasos a seguir:\n",
    "1. Cargar un modelo de BERT sin entrenar.\n",
    "2. Cargar los datos del dataset de El Quijote.\n",
    "3. Aplicar la técnica de Masked Language Modelling (MLM).\n",
    "4. Guardar el modelo entrenado para su posterior uso.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead89e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52dea85",
   "metadata": {},
   "source": [
    "## Cargar un Modelo de BERT sin Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo BERT para Masked Language Modeling\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Verificar que el modelo y el tokenizer se hayan cargado correctamente\n",
    "print(\"Modelo y tokenizer de BERT para MLM se han cargado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el contenido de un archivo de texto plano\n",
    "file_path = 'archivos/quijote.txt'\n",
    "\n",
    "# Cargar el archivo de texto y leer sus líneas\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_lines = file.readlines()\n",
    "\n",
    "# Convertir el contenido del archivo en un formato adecuado para el entrenamiento\n",
    "text_data = [{'text': line.strip()} for line in text_lines if line.strip()]\n",
    "\n",
    "\n",
    "text_df = pd.DataFrame(text_data)\n",
    "print(\"Ejemplo de una instancia del archivo de texto cargado:\")\n",
    "print(text_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac718ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1659f4",
   "metadata": {},
   "source": [
    "## Aplicar Masked Language Modelling (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_with_tokenizer(tokenizer,type_dataset=\"train\"):\n",
    "    # Convertir el array de datos a un objeto Dataset de Hugging Face\n",
    "    if type_dataset==\"train\":\n",
    "        dataset = Dataset.from_list(text_data[100:])\n",
    "    else:\n",
    "        dataset = Dataset.from_list(text_data[:100])\n",
    "\n",
    "    # Tokenizar el texto del dataset\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenizar el texto usando el tokenizer de BERT\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    # Tokenizar el dataset con el método map\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Configurar el DataCollator para Masked Language Modeling (MLM) que enmascara aleatoriamente tokens\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "    return tokenized_dataset, data_collator\n",
    "\n",
    "tokenized_dataset_train, data_collator = generate_dataset_with_tokenizer(bert_tokenizer, type_dataset=\"train\")\n",
    "tokenized_dataset_test, _= generate_dataset_with_tokenizer(bert_tokenizer, type_dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc0552",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510648b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay una GPU disponible y configurar el dispositivo\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Dispositivo utilizado para el entrenamiento: {device}\")\n",
    "\n",
    "#model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "#model.to(device)\n",
    "\n",
    "# Configurar los argumentos para el entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./CursoGenAI_UAL_04_BERT_MLM_Model_example\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=10,\n",
    "    report_to='none'  # Evitar reportes innecesarios si solo estamos probando\n",
    ")\n",
    "\n",
    "# Configurar el Trainer para el entrenamiento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_test\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d429d24",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6cc0c",
   "metadata": {},
   "source": [
    "## Carga del archivo de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el contenido de un archivo de texto plano\n",
    "file_path = 'archivos/tormes.txt'\n",
    "\n",
    "# Cargar el archivo de texto y leer sus líneas\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_lines = file.readlines()\n",
    "\n",
    "# Convertir el contenido del archivo en un formato adecuado para el entrenamiento\n",
    "text_data_tormes = [{'text': line.strip()} for line in text_lines if line.strip()]\n",
    "\n",
    "\n",
    "text_df = pd.DataFrame(text_data_tormes)\n",
    "print(\"Ejemplo de una instancia del archivo de texto cargado:\")\n",
    "print(text_df.head())\n",
    "\n",
    "def generate_dataset_with_tokenizer(tokenizer,data, type_dataset=\"train\"):\n",
    "    # Convertir el array de datos a un objeto Dataset de Hugging Face\n",
    "    if type_dataset==\"train\":\n",
    "        dataset = Dataset.from_list(text_data[100:])\n",
    "    else:\n",
    "        dataset = Dataset.from_list(text_data[:100])\n",
    "\n",
    "    # Tokenizar el texto del dataset\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenizar el texto usando el tokenizer de BERT\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    # Tokenizar el dataset con el método map\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Configurar el DataCollator para Masked Language Modeling (MLM) que enmascara aleatoriamente tokens\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "    return tokenized_dataset, data_collator\n",
    "\n",
    "tokenized_dataset_train_tormes, data_collator_tormes = generate_dataset_with_tokenizer(bert_tokenizer,text_data_tormes, type_dataset=\"train\")\n",
    "tokenized_dataset_test_tormes, _= generate_dataset_with_tokenizer(bert_tokenizer,text_data_tormes, type_dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881ddfb",
   "metadata": {},
   "source": [
    "### Ejercicio 1: \n",
    "Entrenar BERT desde cero\n",
    "\n",
    "Guárdalo en el directorio: 'CursoGenAI_UAL_04_BERT_MLM_Model_scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar los argumentos para el entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./CursoGenAI_UAL_04_BERT_MLM_Model_scratch\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=10,\n",
    "    report_to='none'  # Evitar reportes innecesarios si solo estamos probando\n",
    ")\n",
    "\n",
    "# Configurar el Trainer para el entrenamiento\n",
    "trainer_scratch = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator_tormes,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_test_tormes\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcca47",
   "metadata": {},
   "source": [
    "### Ejercicio 2:\n",
    "\n",
    "Hacer un fine-tuning de BERT pre-entrenado.\n",
    "\n",
    "¿Qué parámetros del bucle de entrenamiento hay que modificar para que se haga un fine-tuning?\n",
    "\n",
    "Principalmente, el parámetro *learning rate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66bf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_finetune = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(bert_model_finetune)\n",
    "# Congelar todas las capas excepto las últimas (capas más específicas)\n",
    "for name, param in model.named_parameters():\n",
    "    # Congelar las capas iniciales (puedes ajustar el rango según lo que quieras congelar)\n",
    "    if 'encoder.layer' in name and int(name.split('.')[3]) < 8:  # Congelar las primeras 8 capas\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Imprimir el estado de los parámetros para verificar\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {\"No entrenable\" if not param.requires_grad else \"Entrenable\"}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cb291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar los argumentos para el fine-tuning\n",
    "training_args_finetune = TrainingArguments(\n",
    "    output_dir=\"./CursoGenAI_UAL_04_BERT_MLM_Model_fine_tuning\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=10,\n",
    "    report_to='none',\n",
    "    learning_rate=5e-5  # Cambia el valor del learning rate aquí \n",
    ")\n",
    "\n",
    "# Configurar el Trainer para el fine-tuning\n",
    "trainer_finetune = Trainer(\n",
    "    model=bert_model_finetune,\n",
    "    args=training_args_finetune,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_test_tormes\n",
    ")\n",
    "\n",
    "# Fine-tuning del modelo pre-entrenado\n",
    "trainer_finetune.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d101cc",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Evaluación de configuraciones\n",
    "\n",
    "Vas a evaluar los tres métodos en un archivo de texto dado. ¿Cuál es el que mejor funciona?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "# Evaluar la exactitud para cada modelo (entrenado desde cero, fine-tuning y preentrenado)\n",
    "bert_model_pretrained = BertForMaskedLM.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "bert_tokenizer_pretrained = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Exactitud del modelo pre-entrenado: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar la exactitud para cada modelo (entrenado desde cero, fine-tuning y preentrenado)\n",
    "bert_model_pretrained = AutoModelForMaskedLM.from_pretrained('CursoGenAI_UAL_04_BERT_MLM_Model_scratch')\n",
    "bert_tokenizer_pretrained = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "eval_results = trainer_scratch.evaluate()\n",
    "\n",
    "print(f\"Exactitud del modelo entrenado desde el principio: {eval_results:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar la exactitud para cada modelo (entrenado desde cero, fine-tuning y preentrenado)\n",
    "bert_model_pretrained = AutoModelForMaskedLM.from_pretrained('CursoGenAI_UAL_04_BERT_MLM_Model_fine_tuning')\n",
    "bert_tokenizer_pretrained = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "eval_results = trainer_finetune.evaluate()\n",
    "\n",
    "print(f\"Exactitud del modelo tas haber hecho fine-tuning: {eval_results:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
