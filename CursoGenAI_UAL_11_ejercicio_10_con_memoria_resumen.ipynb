{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementación de un ChatBot con memoria y resumen de la conversación\n",
        "\n",
        "En esta práctica se implementa un ChatBot con memoria, haciendo uso de Gradio para la visualización de una interfaz gráfica, y de la API de OpenAI para responder las diferentes preguntas o cuestiones del usuario. \n",
        "\n",
        "Para implementar la memoria, se concatenan en un vector las preguntas y respuestas del usuario y del ChatGPT, de tal forma que con cada nueva pregunta del usuario, se incorpora también toda la conversación. Además de esto, se realiza un resumen de la conversación para evitar superar el límite de tokens de entrada. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resuelto por: Javier Navarro Lázaro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "50fa7ce9-1150-42ce-a49a-faae69a0c5d7",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfrYfBuutO9b",
        "outputId": "b0128e30-623b-4876-948c-fb8511c195c0"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install python-dotenv\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d0421a80-0bdb-45ff-8b5f-6f4674d971fe",
          "showTitle": false,
          "title": ""
        },
        "id": "d-dWw3TNsaCB"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configurar la API de Azure OpenAI\n",
        "openai.api_type = os.getenv(\"AZURE_OPENAI_API_TYPE\")\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creación de un chatbot con memoria y resumen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cd58a2eb-d212-417c-9596-b05b084a65c5",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "d9wmm9B5tR36",
        "outputId": "9f08531a-9cfa-43ee-aef9-1f14c0e8b12c"
      },
      "outputs": [],
      "source": [
        "# Función para manejar las solicitudes del chatbot sin memoria\n",
        "def chatbot_no_memory(message):\n",
        "    # El mensaje se envía sin incluir historial previo\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "\n",
        "    # Añadir el nuevo mensaje del usuario\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del asistente\n",
        "    bot_response = response['choices'][0]['message']['content']\n",
        "\n",
        "    # Devolver solo la respuesta del bot (sin historial)\n",
        "    return bot_response\n",
        "\n",
        "# Función para manejar las solicitudes del chatbot sin memoria\n",
        "def chatbot_memory_full(message, chat_history):\n",
        "    # El mensaje se envía sin incluir historial previo\n",
        "\n",
        "    previous_msgs_str = \"\"\n",
        "    for msg in chat_history:\n",
        "      previous_msgs_str += msg[0]\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "\n",
        "    # Añadir el nuevo mensaje del usuario\n",
        "    messages.append({\"role\": \"user\", \"content\": previous_msgs_str+message})\n",
        "\n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del asistente\n",
        "    bot_response = response['choices'][0]['message']['content']\n",
        "\n",
        "    # Devolver solo la respuesta del bot (sin historial)\n",
        "    return bot_response\n",
        "\n",
        "def chatbot_memory_last3(message, chat_history):\n",
        "    # El mensaje se envía sin incluir historial previo\n",
        "\n",
        "    previous_msgs_str = \"\"\n",
        "    for msg in chat_history:\n",
        "      previous_msgs_str += msg[0]\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "\n",
        "    # Añadir el nuevo mensaje del usuario\n",
        "    messages.append({\"role\": \"user\", \"content\": previous_msgs_str+message})\n",
        "\n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del asistente\n",
        "    bot_response = response['choices'][0]['message']['content']\n",
        "\n",
        "    # Devolver solo la respuesta del bot (sin historial)\n",
        "    return bot_response\n",
        "\n",
        "def chatbot_memory_sumup(message, chat_history):\n",
        "    # El mensaje se envía sin incluir historial previo\n",
        "\n",
        "    previous_msgs_str = \"\"\n",
        "    for msg in chat_history[-3:]:\n",
        "      previous_msgs_str += msg[0]\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": \"Resume los siguientes mensajes: \"+previous_msgs_str})\n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response_sumup = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    bot_response_sumup = response_sumup['choices'][0]['message']['content']\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "    # Añadir el nuevo mensaje del usuario\n",
        "    messages.append({\"role\": \"user\", \"content\": bot_response_sumup+message})\n",
        "    print(bot_response_sumup)\n",
        "\n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del asistente\n",
        "    bot_response = response['choices'][0]['message']['content']\n",
        "\n",
        "    # Devolver solo la respuesta del bot (sin historial)\n",
        "    return bot_response\n",
        "\n",
        "# Definir la interfaz de Gradio\n",
        "def create_chat_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Chatbot sin Memoria usando OpenAI\")\n",
        "\n",
        "        # Definir los componentes del chat\n",
        "        chatbot = gr.Chatbot(label=\"Chat con OpenAI\")\n",
        "        msg = gr.Textbox(label=\"Tu mensaje\")\n",
        "        clear = gr.Button(\"Limpiar Chat\")\n",
        "\n",
        "        # Estado del historial de chat\n",
        "        state = gr.State([])  # Estado para almacenar el historial del chat\n",
        "\n",
        "        # Acción para enviar mensaje\n",
        "        def send_message(user_message, chat_history):\n",
        "            # Obtener la respuesta del chatbot sin utilizar historial\n",
        "            # bot_message = chatbot_no_memory(user_message)\n",
        "            #bot_message = chatbot_memory_full(user_message, chat_history)\n",
        "            bot_message = chatbot_memory_sumup(user_message, chat_history)\n",
        "\n",
        "            # Actualizar el historial de chat visible, pero sin afectar la memoria del bot\n",
        "            chat_history.append((user_message, bot_message))\n",
        "\n",
        "            # Devolver el mensaje del usuario y la respuesta acumulada en el historial\n",
        "            return \"\", chat_history, chat_history\n",
        "\n",
        "        # Conectar los componentes\n",
        "        msg.submit(send_message, [msg, state], [msg, state, chatbot])\n",
        "        clear.click(lambda: None, None, chatbot, queue=False)  # Limpiar el chat\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejecución del chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar la interfaz\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_chat_interface()\n",
        "    demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {},
      "notebookName": "CursoGenAI_UAL_11_ejercicio_10_sin_memoria_notebook",
      "widgets": {}
    },
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
