{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementación de un ChatBot con memoria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta práctica se implementa un ChatBot con memoria, haciendo uso de Gradio para la visualización de una interfaz gráfica, y de la API de OpenAI para responder las diferentes preguntas o cuestiones del usuario. \n",
        "\n",
        "Para implementar la memoria, se concatenan en un vector las preguntas y respuestas del usuario y del ChatGPT, de tal forma que con cada nueva pregunta del usuario, se incorpora también toda la conversación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfrYfBuutO9b",
        "outputId": "a3b98cf3-030f-4c7c-fed6-6430daf120d5"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install python-dotenv\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d-dWw3TNsaCB"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener las variables de entorno con los endpoints y claves.\n",
        "openai.api_type = os.getenv(\"AZURE_OPENAI_API_TYPE\")\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creación de un chatbot con memoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para manejar las solicitudes del chatbot con memoria\n",
        "def chatbot_with_memory(message, chat_history):\n",
        "    # Convertir la historia de chat en un formato adecuado para OpenAI\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "    \n",
        "    for user_msg, bot_msg in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "    \n",
        "    # Añadir el nuevo mensaje del usuario\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "    \n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del asistente\n",
        "    bot_response = response['choices'][0]['message']['content']\n",
        "    \n",
        "    # Actualizar la historia del chat con el nuevo par usuario-bot\n",
        "    chat_history.append((message, bot_response))  # Añadir nuevo mensaje al final del historial\n",
        "    \n",
        "    # Devolver el historial actualizado\n",
        "    return chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "d9wmm9B5tR36",
        "outputId": "c4369dc5-8c13-4f3b-942d-46a3d9718d66"
      },
      "outputs": [],
      "source": [
        "# Definir la interfaz de Gradio\n",
        "def create_chat_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Chatbot con Memoria usando OpenAI\")\n",
        "        \n",
        "        # Definir los componentes del chat\n",
        "        chatbot = gr.Chatbot(label=\"Chat con OpenAI\")\n",
        "        msg = gr.Textbox(label=\"Tu mensaje\")\n",
        "        clear = gr.Button(\"Limpiar Chat\")\n",
        "        \n",
        "        # Estado del historial de chat\n",
        "        state = gr.State([])  # Estado para almacenar el historial del chat\n",
        "        \n",
        "        # Acción para enviar mensaje\n",
        "        def send_message(user_message, chat_history):\n",
        "            # Obtener la respuesta del chatbot con la memoria acumulada\n",
        "            chat_history = chatbot_with_memory(user_message, chat_history)\n",
        "            \n",
        "            # Actualizar el chatbot con el historial completo de la conversación\n",
        "            return \"\", chat_history, chat_history\n",
        "        \n",
        "        # Conectar los componentes\n",
        "        msg.submit(send_message, [msg, state], [msg, state, chatbot])\n",
        "        clear.click(lambda: None, None, chatbot, queue=False)  # Limpiar el chat\n",
        "        \n",
        "    return demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejecución del chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar la interfaz\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_chat_interface()\n",
        "    demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
