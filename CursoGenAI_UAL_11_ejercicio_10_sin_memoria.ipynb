{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementación de un ChatBot sin memoria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta práctica se implementa un ChatBot sin memoria, haciendo uso de Gradio para la visualización de una interfaz gráfica, y de la API de OpenAI para responder las diferentes preguntas o cuestiones del usuario. \n",
        "\n",
        "Sin embargo, en este caso, no se incorpora memoria en el chatbot, haciendo llamadas aisladas al SDK de OpenAI. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfrYfBuutO9b",
        "outputId": "a3b98cf3-030f-4c7c-fed6-6430daf120d5"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install python-dotenv\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d-dWw3TNsaCB"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener las variables de entorno con los endpoints y claves.\n",
        "openai.api_type = os.getenv(\"AZURE_OPENAI_API_TYPE\")\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creación del chatbot sin memoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "d9wmm9B5tR36",
        "outputId": "c4369dc5-8c13-4f3b-942d-46a3d9718d66"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Función para manejar las solicitudes del chatbot sin memoria\n",
        "def chatbot_no_memory(message):\n",
        "    # El mensaje se envía sin incluir historial previo\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Eres un asistente útil y amigable.\"}]\n",
        "\n",
        "    # Añadir el nuevo mensaje del usuario\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Solicitud a OpenAI para generar la respuesta\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"gpt-4-turbo\",  # Nombre del deployment\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del asistente\n",
        "    bot_response = response['choices'][0]['message']['content']\n",
        "\n",
        "    # Devolver solo la respuesta del bot (sin historial)\n",
        "    return bot_response\n",
        "\n",
        "# Definir la interfaz de Gradio\n",
        "def create_chat_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Chatbot sin Memoria usando OpenAI\")\n",
        "\n",
        "        # Definir los componentes del chat\n",
        "        chatbot = gr.Chatbot(label=\"Chat con OpenAI\")\n",
        "        msg = gr.Textbox(label=\"Tu mensaje\")\n",
        "        clear = gr.Button(\"Limpiar Chat\")\n",
        "\n",
        "        # Estado del historial de chat\n",
        "        state = gr.State([])  # Estado para almacenar el historial del chat\n",
        "\n",
        "        # Acción para enviar mensaje\n",
        "        def send_message(user_message, chat_history):\n",
        "            # Obtener la respuesta del chatbot sin utilizar historial\n",
        "            bot_message = chatbot_no_memory(user_message)\n",
        "\n",
        "            # Actualizar el historial de chat visible, pero sin afectar la memoria del bot\n",
        "            chat_history.append((user_message, bot_message))\n",
        "\n",
        "            # Devolver el mensaje del usuario y la respuesta acumulada en el historial\n",
        "            return \"\", chat_history, chat_history\n",
        "\n",
        "        # Conectar los componentes\n",
        "        msg.submit(send_message, [msg, state], [msg, state, chatbot])\n",
        "        clear.click(lambda: None, None, chatbot, queue=False)  # Limpiar el chat\n",
        "\n",
        "    return demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejecución del chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ejecutar la interfaz\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_chat_interface()\n",
        "    demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
