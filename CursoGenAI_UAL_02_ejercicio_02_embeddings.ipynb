{"cells":[{"cell_type":"markdown","id":"f9eb5b20","metadata":{"id":"f9eb5b20"},"source":["# Comparación de Embeddings Preentrenados de Hugging Face"]},{"cell_type":"markdown","id":"3596c71c","metadata":{"id":"3596c71c"},"source":["\n","En este notebook, utilizaremos varios modelos de embeddings preentrenados de Hugging Face. Nuestro objetivo es encontrar las palabras más similares a unas dadas utilizando diferentes modelos de embeddings.\n","\n","**Modelos que utilizaremos:**\n","- BERT\n","- RoBERTa\n","- DistilBERT\n","\n","Exploraremos cómo cada modelo interpreta las palabras y calcularemos las similitudes para encontrar las palabras más cercanas.\n","    "]},{"cell_type":"code","execution_count":1,"id":"043e5f42","metadata":{"executionInfo":{"elapsed":16270,"status":"ok","timestamp":1728975720863,"user":{"displayName":"Marcos Lupión Lorente","userId":"16624860811198797627"},"user_tz":-120},"id":"043e5f42"},"outputs":[],"source":["from tqdm import tqdm\n","from transformers import AutoModel, AutoTokenizer\n","import torch\n","from sklearn.metrics.pairwise import cosine_similarity\n"]},{"cell_type":"markdown","id":"5123a2a8","metadata":{"id":"5123a2a8"},"source":["## Cargar Modelos de Embeddings Preentrenados"]},{"cell_type":"code","execution_count":16,"id":"e2d07870","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1728977214375,"user":{"displayName":"Marcos Lupión Lorente","userId":"16624860811198797627"},"user_tz":-120},"id":"e2d07870","outputId":"1d964f36-52ff-4c8b-c1a7-d1b410773976"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Definir los modelos a utilizar\n","models = {\n","    \"BERT\": \"bert-base-uncased\",\n","    \"RoBERTa\": \"roberta-base\",\n","    \"DistilBERT\": \"distilbert-base-uncased\"\n","}\n","models = {\n","    \"BERT\": \"bert-base-uncased\"\n","}\n","\n","# Cargar los modelos y tokenizers\n","loaded_models = {name: AutoModel.from_pretrained(model) for name, model in models.items()}\n","tokenizers = {name: AutoTokenizer.from_pretrained(model) for name, model in models.items()}"]},{"cell_type":"markdown","id":"e4b6c3da","metadata":{"id":"e4b6c3da"},"source":["## Extracción de Embeddings para Palabras Específicas"]},{"cell_type":"code","execution_count":20,"id":"c1a8fcd3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1728977273343,"user":{"displayName":"Marcos Lupión Lorente","userId":"16624860811198797627"},"user_tz":-120},"id":"c1a8fcd3","outputId":"c8d9a7d1-1edd-4518-ad14-98a624c056de"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[ 101, 2632, 5017, 2401,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n","Embedding obtenido para la palabra 'Almería' con el modelo BERT.\n","El embedding tiene dimensiones: torch.Size([5, 768])\n","{'input_ids': tensor([[  101, 28678,   102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n","Embedding obtenido para la palabra 'Banco' con el modelo BERT.\n","El embedding tiene dimensiones: torch.Size([3, 768])\n"]}],"source":["def get_word_embedding(word, model, tokenizer):\n","    # Tokenizar la palabra y convertirla en tensores\n","    inputs = tokenizer(word, return_tensors='pt')\n","    print(inputs)\n","    outputs = model(**inputs)\n","    # Extraer el embedding de la última capa oculta\n","    return outputs.last_hidden_state.squeeze().detach()\n","\n","# Obtener los embeddings para las palabras \"Almería\" y \"Banco\"\n","words = [\"Almería\", \"Banco\"]\n","embeddings = {word: {} for word in words}\n","\n","for word in words:\n","    for name, model in loaded_models.items():\n","        embedding = get_word_embedding(word, model, tokenizers[name])\n","        embeddings[word][name] = embedding\n","        print(f\"Embedding obtenido para la palabra '{word}' con el modelo {name}.\")\n","        print(f\"El embedding tiene dimensiones: {embedding.shape}\")"]},{"cell_type":"markdown","id":"d9d761f1","metadata":{"id":"d9d761f1"},"source":["## Cálculo de Similitudes y Palabras Más Similares"]},{"cell_type":"code","execution_count":null,"id":"f1ee09b3","metadata":{"id":"f1ee09b3"},"outputs":[],"source":["def find_most_similar_words(word_embedding, model_name, tokenizer, top_n=10):\n","    # Obtener todas las palabras del vocabulario del tokenizer\n","    vocab = list(tokenizer.get_vocab().keys())\n","    similarities = []\n","\n","    # Calcular la similitud coseno entre la palabra dada y cada palabra del vocabulario\n","    for token in vocab:\n","        token_embedding = get_word_embedding(token, loaded_models[model_name], tokenizer)\n","        similarity = cosine_similarity([word_embedding.numpy()], [token_embedding.numpy()])[0][0]\n","        similarities.append((token, similarity))\n","\n","    # Ordenar las palabras por similitud descendente y devolver las top_n palabras más similares\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n","    return similarities[:top_n]\n","\n","# Encontrar las palabras más similares para \"Almería\" y \"Banco\" con cada modelo\n","for word in words:\n","    print(f\"Palabras más similares a '{word}':\")\n","    for model_name in models.keys():\n","        similar_words = find_most_similar_words(embeddings[word][model_name], model_name, tokenizers[model_name])\n","        print(f\"Modelo: {model_name}\")\n","        for similar_word, score in similar_words:\n","            print(f\"{similar_word}: {score:.4f}\")\n","    print(\"=\"*50)"]},{"cell_type":"markdown","id":"a939b7dd","metadata":{"id":"a939b7dd"},"source":["# Ejercicio\n","\n","### Ejercicio 1\n","¿Por qué tarda tanto en obtener las palabras similares? Optimiza el código para que se ejecute mucho más rápido.\n","Pista: ¿Cada cuánto se ejecuta el modelo de obtención de embeddings?"]},{"cell_type":"markdown","id":"c0e628f5","metadata":{"id":"c0e628f5"},"source":["Porque se van obteniendo los embeddings uno a uno, en lugar de haciendo uso de un batch de datos."]},{"cell_type":"code","execution_count":26,"id":"TA56SOHnc4fu","metadata":{"executionInfo":{"elapsed":573,"status":"ok","timestamp":1728977812857,"user":{"displayName":"Marcos Lupión Lorente","userId":"16624860811198797627"},"user_tz":-120},"id":"TA56SOHnc4fu"},"outputs":[],"source":["def get_word_batch_embedding(tokens, model, tokenizer):\n","    batch_size = 16  # Comienza con un tamaño de batch seguro\n","    max_sequence_length = 512  # Longitud máxima para la mayoría de los modelos como BERT o GPT-2\n","    print(\"Entra aqui\")\n","    res = []\n","    # Dividir el texto en batches de tamaño adecuado\n","    for i in tqdm(range(0, len(tokens), batch_size * max_sequence_length)):\n","        batch = tokens[i:i + batch_size * max_sequence_length]\n","        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            print(\"Se consulta el modelo\")\n","            # Usar la salida de la última capa oculta como los embeddings\n","            embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Usar solo el token [CLS] para cada secuencia\n","        for i in range(embeddings.shape[0]):\n","            res.append(embeddings[i])\n","    return res"]},{"cell_type":"code","execution_count":null,"id":"W1I-PkyMCKSf","metadata":{"id":"W1I-PkyMCKSf"},"outputs":[],"source":["import numpy as np\n","\n","def find_most_similar_words(word_embedding, model_name, tokenizer, top_n=10):\n","    # Obtener todas las palabras del vocabulario del tokenizer\n","    vocab = list(tokenizer.get_vocab().keys())\n","    similarities = []\n","\n","    embeddings = get_word_batch_embedding(vocab, loaded_models[model_name], tokenizer)\n","\n","    # Calcular la similitud coseno entre la palabra dada y cada palabra del vocabulario\n","    for t in range(len(vocab)):\n","        token = vocab[t]\n","        token_embedding = embeddings[t]\n","\n","        if word_embedding.numpy().shape[0] > 1:\n","          # Hacer las 5 similitudes de coseno individuales\n","          cosine_sim_values = []\n","          for i in range(5):\n","              single_word_embedding = word_embedding[i, :]\n","              similarity = cosine_similarity([single_word_embedding], [token_embedding])[0][0]\n","              cosine_sim_values.append(similarity)\n","          # Calcular la media de las 5 similitudes\n","          similarity = np.mean(cosine_sim_values)\n","        else:\n","          similarity = cosine_similarity([word_embedding.numpy()], [token_embedding])[0][0]\n","        similarities.append((token, similarity))\n","\n","    # Ordenar las palabras por similitud descendente y devolver las top_n palabras más similares\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n","    return similarities[:top_n]\n","\n","\n","words = [\"Almería\", \"Banco\"]\n","\n","# Encontrar las palabras más similares para \"Almería\" y \"Banco\" con cada modelo\n","for word in words:\n","    print(f\"Palabras más similares a '{word}':\")\n","    for model_name in models.keys():\n","        similar_words = find_most_similar_words(embeddings[word][model_name], model_name, tokenizers[model_name])\n","        print(f\"Modelo: {model_name}\")\n","        for similar_word, score in similar_words:\n","            print(f\"{similar_word}: {score:.4f}\")\n","    print(\"=\"*50)"]},{"cell_type":"markdown","id":"WTj-8ZHjJKNT","metadata":{"id":"WTj-8ZHjJKNT"},"source":["```\n","Palabras más similares a 'Almería':\n","Entra aqui\n"," 25%|██▌       | 1/4 [02:05<06:16, 125.40s/it]Se consulta el modelo\n"," 50%|█████     | 2/4 [04:06<04:05, 122.93s/it]Se consulta el modelo\n"," 75%|███████▌  | 3/4 [06:07<02:02, 122.12s/it]Se consulta el modelo\n","100%|██████████| 4/4 [07:37<00:00, 114.29s/it]Se consulta el modelo\n","\n","Modelo: BERT\n","rihanna: 0.2528\n","immigration: 0.2498\n","botany: 0.2464\n","tq: 0.2462\n","cicero: 0.2460\n","plato: 0.2446\n","##eration: 0.2443\n","somme: 0.2443\n","cello: 0.2436\n","##erative: 0.2434\n","==================================================\n","```"]},{"cell_type":"markdown","id":"1c1832eb","metadata":{"id":"1c1832eb"},"source":["### Ejercicio 2\n","Generar los embeddings para dos frases diferentes, que contengan la misma palabra. Muestra el embedding de dicha palabra. ¿Qué observas?"]},{"cell_type":"code","execution_count":29,"id":"R_GPKN20dCbI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":927,"status":"ok","timestamp":1728978495014,"user":{"displayName":"Marcos Lupión Lorente","userId":"16624860811198797627"},"user_tz":-120},"id":"R_GPKN20dCbI","outputId":"b20bbeac-38fe-416e-b141-3d6cbd9ea611"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([11, 768])\n","Token: [CLS], Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.4796,  0.0542])\n","Token: en, Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.0212, -0.1596])\n","Token: al, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.0306, -0.6478])\n","Token: ##mer, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.5880, -0.3254])\n","Token: ##ia, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.2525, -0.0318])\n","Token: ha, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.0630, -1.0317])\n","Token: ##ce, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.1877, -0.0969])\n","Token: cal, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.6961, -0.3533])\n","Token: ##or, Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.1220,  0.0452])\n","Token: ., Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.2790, -0.4968])\n","Token: [SEP], Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 1.1214, -0.0699])\n","torch.Size([12, 768])\n","Token: [CLS], Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.4785, -0.0456])\n","Token: en, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.1267, -0.2604])\n","Token: al, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.1067, -0.6802])\n","Token: ##mer, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.5782, -0.3487])\n","Token: ##ia, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.2974, -0.1021])\n","Token: no, Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.1256, -0.5482])\n","Token: ha, Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.0060, -0.9504])\n","Token: ##ce, Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.0319, -0.0864])\n","Token: cal, Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 0.4921, -0.4780])\n","Token: ##or, Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.3543,  0.0145])\n","Token: ., Embedding shape: torch.Size([768]), Primeros digitos = tensor([-0.1632, -0.6953])\n","Token: [SEP], Embedding shape: torch.Size([768]), Primeros digitos = tensor([ 1.1256, -0.0524])\n"]}],"source":["# Cargar el modelo preentrenado y el tokenizador\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","\n","# Texto de entrada\n","\n","texts = [\"En Almería hace calor.\", \"En Almería no hace calor.\"]\n","\n","for text in texts:\n","  # Tokenizar el texto\n","  inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n","\n","  # Pasar los inputs por el modelo para obtener los embeddings\n","  with torch.no_grad():\n","      outputs = model(**inputs)\n","\n","  # Extraer los embeddings de la última capa\n","  last_hidden_state = outputs.last_hidden_state  # Tensor de tamaño (batch_size, sequence_length, hidden_size)\n","\n","  # Obteniendo los embeddings de cada token\n","  token_embeddings = last_hidden_state.squeeze(0)  # Eliminar la dimensión del batch si solo hay un input\n","  print(token_embeddings.shape)\n","  # Mostramos los tokens y sus correspondientes embeddings\n","  tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n","\n","  for token, embedding in zip(tokens, token_embeddings):\n","      print(f\"Token: {token}, Embedding shape: {embedding.shape}, Primeros digitos = {embedding[0:2]}\")"]},{"cell_type":"code","execution_count":null,"id":"iFOuTkMQJh9N","metadata":{"id":"iFOuTkMQJh9N"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":5}
