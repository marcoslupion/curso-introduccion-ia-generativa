{"cells":[{"cell_type":"markdown","metadata":{"id":"e5JLETRqNyd2"},"source":["# Entrenamiento de un LLM\n","\n","En este notebook se incorpora un ejemplo de entrenamiento de un LLM haciendo uso de una técnica conocida como LowRank para hacer este entrenamiento posible en el hardware disponible, ya que permite entrenar un número muy reducido de nuevos pesos, en comparación con el número de pesos total del modelo. "]},{"cell_type":"markdown","metadata":{"id":"rtL-fHFmWDRB"},"source":["## Instalación e importación de librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21g82Z4lNyd4"},"outputs":[],"source":["!pip install transformers torch datasets peft -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DUuvCIeNyd5"},"outputs":[],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForSeq2SeqLM, TrainingArguments\n","import torch\n","from transformers import Trainer , AutoTokenizer, AutoModelForCausalLM\n","from peft import get_peft_model, PeftModel, PeftConfig, AutoPeftModelForCausalLM, LoraConfig, TaskType"]},{"cell_type":"markdown","metadata":{"id":"kd7soz1XWEqx"},"source":["## Obtención de modelo y tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSU69lJMNyd6"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"deepset/tinyroberta-squad2\")\n","model = AutoModelForCausalLM.from_pretrained(\"deepset/tinyroberta-squad2\")"]},{"cell_type":"markdown","metadata":{"id":"5Xf3dZg_WHhb"},"source":["## Aplicación de LORA para evitar entrenar todo el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVJL9UmmNyd6"},"outputs":[],"source":["peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{"id":"cPomcRf3WMAy"},"source":["## Obtención del dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfEOP-xYSuWU"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Cargar el dataset\n","ds = load_dataset(\"Malikeh1375/medical-question-answering-datasets\", \"all-processed\", split='train')\n","\n","# Seleccionar solo los primeros 100 elementos\n","ds = ds.select(range(100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvY77sVbcrQe"},"outputs":[],"source":["# Verifica el tamaño del nuevo dataset\n","print(len(ds))  # Debería imprimir 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yof0-NamNyd7"},"outputs":[],"source":["def preprocess_function(examples):\n","    # Concatenar 'instruction' con 'input' para formar la entrada del modelo.\n","    inputs = examples['instruction'] + \" \" + examples['input']  # Ajustar si la concatenación es diferente\n","    outputs = examples['output']  # La salida es la columna 'output'\n","\n","    # Tokenizar la entrada (inputs)\n","    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n","\n","    # Tokenizar la salida (outputs)\n","    with tokenizer.as_target_tokenizer():  # Asegurarse de que el tokenizador procese bien las etiquetas\n","        labels = tokenizer(outputs, padding=\"max_length\", truncation=True, max_length=128)\n","\n","    # Reemplazar los tokens de padding en los labels por -100 (ignorados en la pérdida)\n","    labels[\"input_ids\"] = [(label if label != tokenizer.pad_token_id else -100) for label in labels[\"input_ids\"]]\n","\n","    # Añadir las etiquetas al diccionario de las entradas\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n","\n","# Preprocesar el dataset tokenizándolo\n","tokenized_ds = ds.map(preprocess_function, batched=False)"]},{"cell_type":"markdown","metadata":{"id":"uzq2g8niWP3b"},"source":["## Definición del bucle de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iu5RZKR-Nyd6"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=1e-3,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kC9pEM_XY55u"},"outputs":[],"source":["from transformers import Trainer\n","\n","# Definir el Trainer con el dataset ya tokenizado\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_ds,  # Usamos el dataset tokenizado\n","    tokenizer=tokenizer  # Tokenizador para asegurarse de que el modelo está alineado con los tokens\n",")\n","\n","# Entrenar el modelo\n","trainer.train()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
