{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77538cc6",
   "metadata": {},
   "source": [
    "# Generación de Embeddings y Evaluación de Similitud de Frases con S-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c65cc",
   "metadata": {},
   "source": [
    "\n",
    "En este notebook, utilizaremos un modelo S-BERT (Sentence-BERT) pre-entrenado para generar embeddings de frases y evaluar la similitud entre ellas. También evaluaremos el rendimiento del modelo S-BERT utilizando datasets conocidos como STSB (Semantic Textual Similarity Benchmark) y MNLI (Multi-Genre Natural Language Inference).\n",
    "\n",
    "**Objetivos del notebook:**\n",
    "1. Cargar un modelo S-BERT pre-entrenado para generar embeddings de frases.\n",
    "2. Calcular la similitud de coseno entre frases aleatorias y analizar cuáles son más similares.\n",
    "3. Evaluar el rendimiento del modelo S-BERT utilizando los datasets STSB o MNLI.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0b653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías necesarias si no están ya instaladas\n",
    "!pip install sentence-transformers scikit-learn -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0b627",
   "metadata": {},
   "source": [
    "## Cargar un modelo S-BERT pre-entrenado y generar embeddings de frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Cargar un modelo S-BERT pre-entrenado\n",
    "sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Lista de frases aleatorias para generar embeddings\n",
    "frases = [\n",
    "    \"El cielo está despejado y azul.\",\n",
    "    \"La tecnología avanza rápidamente.\",\n",
    "    \"Es un día perfecto para un paseo.\",\n",
    "    \"Es un día ideal para un paseo.\",\n",
    "    \"El desarrollo de software es una habilidad importante.\",\n",
    "    \"Me gusta ver películas en mi tiempo libre.\"\n",
    "]\n",
    "\n",
    "# Generar los embeddings para las frases usando S-BERT\n",
    "embeddings = sbert_model.encode(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los embeddings generados\n",
    "print(\"Embeddings generados para las frases:\")\n",
    "print(embeddings)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef528c",
   "metadata": {},
   "source": [
    "## Calcular la similitud de coseno entre las frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1ce5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Similitud de Coseno entre las frases:\n",
      "[[0.99999994 0.43827826 0.52646255 0.53450996 0.44253314 0.48676413]\n",
      " [0.43827826 0.9999997  0.4820129  0.50022745 0.40867415 0.4140717 ]\n",
      " [0.52646255 0.4820129  1.0000001  0.8669213  0.48401302 0.62186503]\n",
      " [0.53450996 0.50022745 0.8669213  0.9999998  0.5409801  0.64951175]\n",
      " [0.44253314 0.40867415 0.48401302 0.5409801  1.0000001  0.4044858 ]\n",
      " [0.48676413 0.4140717  0.62186503 0.64951175 0.4044858  1.        ]]\n",
      "Las frases más similares son: 'Es un día perfecto para un paseo.' y 'Es un día ideal para un paseo.' con una similitud de 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calcular la matriz de similitud de coseno entre las frases\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Mostrar la matriz de similitud\n",
    "print(\"Matriz de Similitud de Coseno entre las frases:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Encontrar las frases más similares (excluyendo la diagonal)\n",
    "np.fill_diagonal(similarity_matrix, 0)\n",
    "most_similar_pairs = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n",
    "print(f\"Las frases más similares son: '{frases[most_similar_pairs[0]]}' y '{frases[most_similar_pairs[1]]}' con una similitud de {similarity_matrix[most_similar_pairs]:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe98c4",
   "metadata": {},
   "source": [
    "## Evaluar el modelo S-BERT utilizando los datasets STSB o MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453a6293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson_cosine': 0.7058473426134255, 'spearman_cosine': 0.7103037997967742, 'pearson_manhattan': 0.7064111012872616, 'spearman_manhattan': 0.7052518876846171, 'pearson_euclidean': 0.7109221058831185, 'spearman_euclidean': 0.7103037997967742, 'pearson_dot': 0.7058473368780483, 'spearman_dot': 0.7103037997967742, 'pearson_max': 0.7109221058831185, 'spearman_max': 0.7103037997967742}\n",
      "Rendimiento del modelo S-BERT en el dataset STSB: 0.7058\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import evaluation, SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset STSB para evaluar la similitud textual\n",
    "stsb_dataset = load_dataset('stsb_multi_mt', 'es', split='dev')\n",
    "\n",
    "# Preparar los textos y las etiquetas del dataset para evaluación\n",
    "sentences1 = stsb_dataset['sentence1']\n",
    "sentences2 = stsb_dataset['sentence2']\n",
    "labels = stsb_dataset['similarity_score']\n",
    "\n",
    "# Crear un evaluador de S-BERT para medir el rendimiento del modelo en el dataset STSB\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, labels)\n",
    "score = evaluator(sbert_model)\n",
    "print(score)\n",
    "print(f\"Rendimiento del modelo S-BERT en el dataset STSB: {score['pearson_cosine']:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e88d4",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf6e25",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Piensa una pregunta, y un conjunto de respuestas que se puedan clasificar en varios tipos:\n",
    "- Que contestan bien a la pregunta\n",
    "- Que contestan muy mal a la pregunta\n",
    "- Que no tienen nada que ver\n",
    "\n",
    "Ejecuta S-BERT para obtener los embeddings tanto de la pregunta, como de las respuestas. \n",
    "Calcula la \"cosine-similarity\" entre la pregunta y las respuestas. Escoge la respuesta que tenga menos valor. \n",
    "¿Es la respuesta correcta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c844bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marcos\\Programas\\Python3.8\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities: tensor([[0.8436, 0.7397, 0.1346, 0.5989]])\n",
      "Response with the highest similarity: The capital of Spain is Madrid.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "question = \"What is the capital of Spain?\"\n",
    "\n",
    "responses = [\n",
    "    \"The capital of Spain is Madrid.\",  # Respuesta correcta\n",
    "    \"The capital of Spain is Berlin.\",  # Respuesta incorrecta\n",
    "    \"Bananas are yellow and sweet.\",    # No tiene nada que ver\n",
    "    \"Madrid is a beautiful city in Europe.\"  # Es relevante pero no es una respuesta directa. \n",
    "]\n",
    "\n",
    "question_embedding = sbert_model.encode(question, convert_to_tensor=True)\n",
    "responses_embeddings = sbert_model.encode(responses, convert_to_tensor=True)\n",
    "\n",
    "cosine_similarities = util.pytorch_cos_sim(question_embedding, responses_embeddings)\n",
    "\n",
    "lowest_similarity_idx = cosine_similarities.argmax().item()\n",
    "\n",
    "lowest_similarity_response = responses[lowest_similarity_idx]\n",
    "\n",
    "print(\"Cosine similarities:\", cosine_similarities)\n",
    "print(\"Response with the highest similarity:\", lowest_similarity_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
